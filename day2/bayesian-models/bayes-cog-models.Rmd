---
title: "Bayesian Estimation of Cognitive Models"
author: "Stephen Rhodes and Julia Haaf"
output: 
  ioslides_presentation:
    logo: ../../day1/intro-to-R/pictures/MUlogoRGB.png
    widescreen: true
subtitle: Modeling the individual and group
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Overview

We're going to work through some example models

- **Change detection data (from yesterday)**
- Delayed estimation recall error
- SDT model of confidence rating data

These examples use binomial (accuracy), continuous, and ordinal data, respectively.

# Change Detection

## Rouder et al. (2008) data

The data is structured differently from the MLE example

```{r, echo=T}
cd = read.table(file = "jags-change-det/rouder08-longdata-0.5.dat")
head(cd)
# ischange = was the trial a change trial? 1 = yes, 0 = no
# respchange = the number of trials that the participant said 'change' to
# ntrials = number of trials of that type for that participant in that condition
```

## Rouder et al. (2008)

- See `jags-rouder08.R`
- Reminder of the model:

$$
p(\mbox{resp} = \mbox{change} \mid \mbox{change}) = h =  a(d + (1 - d)g) + (1 - a)g
$$

$$
p(\mbox{resp} = \mbox{change} \mid \mbox{no-change}) = f = a(1 - d)g + (1 - a)g
$$

## In JAGS

```{r, eval=F, echo=T}
      # p(resp = change)
      P[i] <- ifelse(ischange[i] == 1, 
                     a[i]*(d[i]+(1-d[i])*g[i]) + (1-a[i])*g[i], # p(hit)
                     a[i]*(1-d[i])*g[i] + (1-a[i])*g[i]) # p(false-alarm)
```

## Rouder et al. (2008) hierarchical model

Parameters for participant $i$ are drawn from independent normals

$\kappa_i \sim \mbox{Normal}(\mu_\kappa, \sigma_\kappa)$

$k_i = \max(\kappa_i, 0)$ (see [Morey, 2011](http://isiarticles.com/bundles/Article/pre/pdf/71624.pdf))

$\mbox{logit}(a_i) \sim \mbox{Normal}(\mu_a, \sigma_a)$

$\mbox{logit}(g_i) \sim \mbox{Normal}(\mu_g, \sigma_g)$

## logit

$\mbox{logit}(p) = \ln(p/(1-p))$

Maps probabilities onto real numbers (so we can sample participant parameters from normal distributions)

In R `qlogis(x)`. In JAGS `logit(x)`

```{r, fig.height=3}
par(mar=c(4,4,1,1))
curve(qlogis(x), from=0, to=1, main="", xlab="p", ylab="logit(p)")
```

## In JAGS

```{r, eval=F, echo=T}
k[i] <- max(kappa[i], 0) # 'Mass-at-chance' transformation
kappa[i] <- K_s[id[i]]
logit(a[i]) <- A_s[id[i]] # logit transformation
logit(g[i]) <- G_s[id[i]]
```

```{r, eval=F, echo=T}
for (s in 1:S){
  K_s[s] ~ dnorm(K_mu, 1/K_sigma^2)
  A_s[s] ~ dnorm(A_mu, 1/A_sigma^2)
  G_s[s] ~ dnorm(G_mu, 1/G_sigma^2)
}
```

## Priors

$\mu_\kappa \sim \mbox{Normal}(3, 4)$

$\mu_a \sim \mbox{Normal}(2.2, 4)$ (`plogis(2.2)` $\approx$ 0.9)

$\mu_g \sim \mbox{Normal}(0, 4)$ (`plogis(0)` = 0.5)

```{r, eval=F, echo=T}
K_mu ~ dnorm(3, 1/4^2) 
A_mu ~ dnorm(2.2, 1/4^2) 
G_mu ~ dnorm(0, 1/4^2) 
```

## Priors

$\sigma_\kappa \sim \mbox{Gamma}(s, r)$

$\sigma_a \sim \mbox{Gamma}(s, r)$

$\sigma_g \sim \mbox{Gamma}(s, r)$

```{r, eval=F, echo=TRUE}
K_sigma ~ dgamma(shape, rate)
A_sigma ~ dgamma(shape, rate)
G_sigma ~ dgamma(shape, rate)

shape <- 1.01005
rate <- 0.1005012
```

## Priors

```{r}
curve(dgamma(x, shape = 1.01005, rate = 0.1005012), from=0, to=20, n = 10000, ylab="", xlab=bquote(sigma), lwd=2)
```

It is more intuitive to work with priors on SD, but it can lead to better estimation to use inverse gamma priors on variance or a gamma prior on precision

# Delayed Estimation

## Delayed Estimation

- Study items that vary on a continuous (usually circular dimension)
- Reproduce a probed item

## Zhang and Luck (2008)

- Another example using colored squares!

```{r, out.width = "200px", echo=F}
knitr::include_graphics("pictures/zhang-luck.png")
```

## Zhang and Luck (2008) - data

```{r, echo=T}
de = read.table("delayed-estimation/zhang-luck08.dat")

head(de)
```

## Zhang and Luck (2008) - data

```{r, echo=F}
colors = rainbow(4, alpha = .5)
par(mfrow = c(2,2), mar=c(3,3,2,2))
Ns = unique(de$setsize)
Ns = Ns[order(Ns)]

for (i in 1:4){
  N = Ns[i]
  with(subset(de, setsize==N), hist(error, main=paste0("N = ", N), xlab="error (rad)", ylab="", col=colors[i], breaks=30))
}

```

## Zhang and Luck (2008) - models

Responses are a *mixture* of memory responses and random guesses

$p(y = x) = P_{m}\mbox{vonMises}(x; \sigma) + (1 - P_m)\frac{1}{2\pi}$

```{r, fig.height=4, echo=F, messages=F, warning=F}
library(circular, quietly = T, warn.conflicts = F)
de_mixture <- function(y, sd, pmem, mu=0, log=T){
  # delayed estimation mixture
  dvon = suppressWarnings(dvonmises(circular(y), mu = mu, kappa = 1/sd^2)) # suppresses messages about converting data to the circular class 
  p <- pmem*dvon + (1-pmem)*(1/(2*pi))
  if (log){
    return(log(p))
  }else{
    return(p)
  }
}

curve(de_mixture(x, sd = .5, pmem = .8, log=F), from=-pi, to=pi, ylim=c(0,.8), ylab='', xlab="Error (radians)", lwd=2)

curve(de_mixture(x, sd = .5, pmem = .5, log=F), from=-pi, to=pi, ylim=c(0,.8), lwd=2, add = T, col="dodgerblue")

legend('topleft', legend = c(.8, .5), lwd=2, col=c("black", 'dodgerblue'), bty='n', title="Pm")

```

## Zhang and Luck (2008) - models

They also fit a simpler version of the model where all responses are from memory and only the standard deviation of the von Mises (circular normal) distribution varies with set size.

```{r, fig.height=4, echo=F, messages=F, warning=F}

curve(de_mixture(x, sd = .5, pmem = 1, log=F), from=-pi, to=pi, ylim=c(0,.8), ylab='', xlab="Error (radians)", lwd=2)

curve(de_mixture(x, sd = 1, pmem = 1, log=F), from=-pi, to=pi, ylim=c(0,.8), lwd=2, add = T, col="dodgerblue")

legend('topleft', legend = c(.5, 1), lwd=2, col=c("black", 'dodgerblue'), bty='n', title="SD")

```

## In JAGS

See [Oberauer et al. (2016)](https://www.ncbi.nlm.nih.gov/pubmed/28538991)

```{r, eval=F, echo=T}

for (i in 1:n){
  y[i] ~ dvonmises(mu, kappa[i])
  
  kappa[i] <- (1/sd[i]^2)*z[i] # kappa = 0 produces a uniform distribution
  
  z[i] ~ dbern(pm[i]) # 1 = response from memory, 0 = guess
  
  pm[i] <- min(k[i]/N[i], 1)
  k[i] <- max(kap[i], 0)
  kap[i] <- K_s[id[i]]
  sd[i] <- exp(SD_s[id[i], N_i[i]])
}
```

## In JAGS

```{r, eval=F, echo=T}
for (s in 1:S){
  K_s[s] ~ dnorm(K_mu, 1/K_sigma^2)
  for (ss in 1:N_n){
    SD_s[s, ss] ~ dnorm(SD_mu[ss], 1/SD_sigma^2)
  }
}

K_mu ~ dnorm(3, 1/4^2)
for (ss in 1:N_n){
  SD_mu[ss] ~ dnorm(2, 1/10^2)
}

SD_sigma ~ dgamma(shape, rate)
K_sigma ~ dgamma(shape, rate)

shape <- 1.01005 
rate <- 0.1005012
```

## In JAGS

`jags-zhang-luck08.R` runs hierarchical models on this data set. However, you need to install an extension for JAGS to get the von Mises distribution. Those interested can follow the links in the script to get the module.

`mle-zhang-luck08.R` contains functions for MLE. Those interested can also use these to model this data set

## 

For both change detection and (especially) delayed estimation many more complicated models have been proposed. 

See, for example, [van den Berg et al. (2014)](http://www.cns.nyu.edu/malab/static/files/publications/2014%20Van%20den%20Berg%20Ma%20Psych%20Review.pdf)

These models provide the foundations for the more complex versions.

# SDT confidence ratings

```{r}
ratingModel <- function(d, s, a, b, C, lastPoint = T){
  # the parsimonius model from Selker et al
  # https://osf.io/v3b76/
  unb = -log((1 - 1:C/C)/(1:C/C))
  
  thresh = 1/2 + a*unb + b
  
  f = cumsum(rev(diff(pnorm(c(-Inf, thresh),0,1))))
  h = cumsum(rev(diff(pnorm(c(-Inf, thresh),d,s))))
  
  if(!lastPoint){
    f = f[1:(C-1)]
    h = h[1:(C-1)]
  }
  return(cbind(f, h))
}

plotSDT <- function(d = 2, s = 1, a = 1, b = 0, C = 6, title = F){
  par(pty='m', mar=c(4,2,3,2))
  
  xrange <- c(-3, d+3*s)
  newcol = col2rgb("dodgerblue4")[,1]
  newcol = rgb(red = newcol[1], green = newcol[2], blue = newcol[3], alpha = 50, maxColorValue = 255)
  oldcol=rgb(1,1,1,.5)
  xseq = seq(from=xrange[1], to=xrange[2], length.out = 1000)

  unb = -log((1 - 1:C/C)/(1:C/C))
  thresh = 1/2 + a*unb + b
  thresh = thresh[1:(C-1)]
  
  yrange = c(0, max(dnorm(0), dnorm(d, d, s)))
  plot(NA, ylim = yrange*c(0,1.5), xlim = xrange, ylab="", xlab="", axes=F)
  
  polygon(c(xrange[1], xseq, xrange[2]), c(0, dnorm(xseq), 0), col = newcol , border = NA)
  curve(expr = dnorm(x, 0, 1), from=xrange[1], to=xrange[2], n = 10000, add = T)
  polygon(c(xrange[1], xseq, xrange[2]), c(0, dnorm(xseq, d, s), 0), col = oldcol , border = NA)
  curve(expr = dnorm(x, d, s), from=xrange[1], to=xrange[2], n = 10000, add = T)
  
  for (k in 1:(C-1)){
    lines(x = c(thresh[k], thresh[k]), y = yrange*c(0, 1.35), lty=2, col='darkgrey')
    text(x = thresh[k], y = yrange[2]*1.4, labels = bquote(lambda[.(k)]))
  }
  
  label_x = c(xrange[1], thresh, xrange[2])[1:C] + diff(c(xrange[1], thresh, xrange[2]))/2
  
  text(x = label_x, y = yrange[2]*1.2, labels = 1:C, col='red')
  
  axis(1); box()
  
  if (title){
    mtext(text = paste0("d = ", d, ", s = ", s, ", a = ", a, ", b = ", b))
  }
  mtext(text = "Strength of Evidence for 'old'", 1, line = 2.5)
}

# par(pty='s')
# plot(ratingModel(d = 2, s = 1.2, a = 1, b = 0, C = 1000), ylim=c(0,1), xlim=c(0,1), type='l', xlab='false-alarm rate', ylab='hit rate')
# a = ratingModel(d = 2, s = 1.2, a = 1, b = 0, C = 6, lastPoint=F)
# points(a, ylim=c(0,1), xlim=c(0,1), pch=21, bg='grey')
# 
# text(t(t(a)+c(.05,-.05)), labels = 5:1)
#
# par(mfrow=c(1,2))
# 
# plotSDT(d = 2, s = 1.2, a = 1, b = -2/3, C = 6)
# mtext("Shift to the left (more liberal)")
# 
# plotSDT(d = 2, s = 1.2, a = 1, b = 1, C = 6)
# mtext("Shift to the right (more conservative)")
# 
# par(pty='s', mfrow=c(1,2))
# 
# plot(ratingModel(d = 2, s = 1.2, a = 1, b = 0, C = 1000), ylim=c(0,1), xlim=c(0,1), type='l', xlab='false-alarm rate', ylab='hit rate')
# a = ratingModel(d = 2, s = 1.2, a = 1, b = -2/3, C = 6, lastPoint=F)
# points(a, ylim=c(0,1), xlim=c(0,1), pch=21, bg='grey')
# 
# mtext("Shift to the left (more liberal)")
# 
# plot(ratingModel(d = 2, s = 1.2, a = 1, b = 0, C = 1000), ylim=c(0,1), xlim=c(0,1), type='l', xlab='false-alarm rate', ylab='hit rate')
# a = ratingModel(d = 2, s = 1.2, a = 1, b = 1, C = 6, lastPoint=F)
# points(a, ylim=c(0,1), xlim=c(0,1), pch=21, bg='grey')
# mtext("Shift to the right (more conservative)")

```

## Example paradigm (Pratte et al., 2010, JEPLMC)

- Participants study 240 sequentially presented words
- At test they see 480 words (1/2 old/studied, 1/2 new)
- Instead of an old/new judgement, participants give an ordinal response:
- 1 = *sure new*, 2 = *believe new*, 3 = *guess new*, 4 = *guess old*, 5 = *believe old*, 6 = *sure old*.

## Why?

- To separate sensitivity from bias we need to know the ROC curve
- With an old/new decision we only get one point (left)
- Multiple ratings allow us to approximate ROC curves (right)

```{r, fig.height=4}
par(pty='s', mfrow=c(1,2), mar=c(3,4,1,1))
plot(NA, ylim=c(0,1), xlim=c(0,1), type='l', xlab='false-alarm rate', ylab='hit rate', main="Old/New")
a = ratingModel(d = 2, s = 1.2, a = 1, b = 0, C = 6, lastPoint=F)
points(a[3,1], a[3,2], ylim=c(0,1), xlim=c(0,1), pch=21, bg='grey', cex=1.5)

plot(NA, ylim=c(0,1), xlim=c(0,1), type='l', xlab='false-alarm rate', ylab='hit rate', main="Rating 1-6")
points(a, ylim=c(0,1), xlim=c(0,1), pch=21, bg='grey', cex=1.5)
```

## Signal detection model

For the old/new task the signal detection theory model is:

```{r}
plotSDT(d = 2, s = 1.2, a = 1, b = 1, C = 2)
```

## Signal detection model

For the rating task the signal detection theory model is:

```{r}
plotSDT(d = 2, s = 1.2, a = 1, b = .3, C = 6)
```

## Signal detection model
<div class="columns-2">

- The participant is assumed to set up $K - 1$ thresholds (where $K$ is the number of rating categories).
- $p(\mbox{rating} = k \mid \mbox{new}) = \Phi(-\lambda_k) - \Phi(-\lambda_{k - 1})$ ($\Phi$ = `pnorm`)
- $p(\mbox{rating} = k \mid \mbox{old}) = \Phi\left(\frac{d - \lambda_k}{s}\right) - \Phi\left(\frac{d - \lambda_{k - 1}}{s}\right)$
- $\lambda_0 = -\infty$, $\lambda_K = -\infty$
- $d$ = sensitivity, $s$ = SD of the old distribution

</br>
</br>
</br>
</br>

```{r, fig.width=4, fig.height=5, fig.align='right'}
plotSDT(d = 2, s = 1.2, a = 1, b = .3, C = 6)
```

</div>

## Signal detection model

- If the thresholds are freely estimated the number of parameters to be estimated increases as the number of rating options increases.
- [Selker et al. (pre-print)](https://osf.io/b6z8e/) present a way of modeling thresholds with two parameters
    - Start with unbiased thresholds ($\gamma_k$)
    - Scale and shift them with two parameters: $\lambda_k = 1/2 + a\gamma_k + b$
    - Scale = $a$, Shift = $b$

## Selker et al. signal detection model

Scale

```{r}
par(mfrow=c(1,2))
plotSDT(d = 2, s = 1.2, a = 1, b = .3, C = 6, title = T)

plotSDT(d = 2, s = 1.2, a = 1.5, b = .3, C = 6, title = T)
```

## Selker et al. signal detection model

Shift

```{r}
par(mfrow=c(1,2))
plotSDT(d = 2, s = 1.2, a = 1, b = 0, C = 6, title = T)

plotSDT(d = 2, s = 1.2, a = 1, b = 1.5, C = 6, title = T)
```

## Model

$d_i \sim \mbox{Normal}(d_\mu, 1)$

$s_i \sim \mbox{Normal}(s_\mu, 1)$

$a_i \sim \mbox{Normal}(a_\mu, 1)$

$b_i \sim \mbox{Normal}(b_\mu, 1)$

In JAGS (see `HierSDT_model.txt`)

```{r, eval=F, echo=T}
mu[k] ~ dnorm(muMu,1) #
sigma[k] ~ dnorm(sigmaMu, 1)  # unequal-variance (s)
lambda[k] <- 1/(sigma[k]^2) # 1/s^2
```

## Model

Hyperparameters

$d_\mu \sim \mbox{Normal}^+(1, 1)$

$s_\mu \sim \mbox{Normal}(1.1, 1)\mbox{I}(1,5)$

$a_\mu \sim \mbox{Normal}^+(1, 1)$

$a_\mu \sim \mbox{Normal}(0, 1)$

In JAGS (see `HierSDT_model.txt`)

```{r, eval=F, echo=T}
muMu ~ dnorm(1,1) I(0,)
sigmaMu ~ dnorm(1.1,1) I(1,5)	
aMu ~ dnorm(1,1) I(0,)
bMu ~ dnorm(0,1)
```

## Model

This code sets the thresholds ($\lambda_k$)

```{r, eval=F, echo=T}
# Set unbiased thresholds on the [0,1] line and the real line [-∞,∞]
for (c in 1:(nCat-1)) {
	gam[c] <- c/nCat
	gamReal[c] <- -log((1-gam[c])/gam[c])
}


# Use regression function to estimate thresholds on real line
for (c in 1:(nCat-1)) {
	dReal[k, c] <- a[k] * gamReal[c] + b[k] + .5
}

# in the code k refers to participants and c to ratings...
```

## Data

Before talking more about the data it's useful to introduce the structure of the data, which comes from [Pratte et al. (2010)](http://pcn.psychology.msstate.edu/Publications/Pratte_etal_JEPLMC_2010.pdf)

```{r, echo=T}

load("confidence-rating/pratte10.RData")

str(pratte10_list)

```

## Model

New trials

```{r, eval=F, echo=T}
for (i in 1:nNoise[k]) { # for noise items
	pNoise[k,i,1] <- pnorm(dReal[k,1], 0, 1) # gets area under first threshold
	for (c in 2:(nCat-1)) { # gets area between thresholds 2:(nCat-1)
		pNoise[k,i,c] <- pnorm(dReal[k,c], 0, 1) - sum(pNoise[k,i,1:(c-1)])
	}
	pNoise[k,i,nCat] <- 1 - sum(pNoise[k,i,1:(nCat-1)]) # gets area for the last threshold
	xNoise[k,i] ~ dcat(pNoise[k,i,1:nCat]) # likelihood
}
```

## Model

Old trials

```{r, eval=F, echo=T}
for (j in 1:nSignal[k]) {	# for signal items
	pSignal[k,j,1] <- pnorm(dReal[k,1], mu[k], lambda[k]) # gets area under first threshold
	for (c in 2:(nCat-1)) { # gets area between thresholds 2:(nCat-1)
		pSignal[k,j,c] <- pnorm(dReal[k,c], mu[k], lambda[k]) - sum(pSignal[k,j,1:(c-1)])
	}
	pSignal[k,j,nCat] <- 1 - sum(pSignal[k,j,1:(nCat-1)]) # gets area for the last
	xSignal[k,j] ~ dcat(pSignal[k,j,1:nCat]) # likelihood	
}
```

## 

To work with this example go to the `fit-selker-model.R` script

## End

This has been a whirlwind tour of fitting models in `R`

For more detail, here are some great resources (there are many more):

- [Farrell & Lewandowsky (2018) Computational Modeling of Cognition and Behavior](https://www.amazon.com/Computational-Modeling-Cognition-Behavior-Farrell/dp/1107525616/ref=pd_lpo_sbs_14_t_0?_encoding=UTF8&psc=1&refRID=NFD23G008H81Q439Q2QE)
- [Lee & Wagenmakers (2014) Bayesian Cognitive Modeling: A Practical Course](https://www.amazon.com/Bayesian-Cognitive-Modeling-Practical-Course/dp/1107603579/ref=pd_bxgy_14_img_3?_encoding=UTF8&pd_rd_i=1107603579&pd_rd_r=Z05RWKB1HN2NMKPGVRHP&pd_rd_w=eKzQg&pd_rd_wg=XyvbI&psc=1&refRID=Z05RWKB1HN2NMKPGVRHP&dpID=51QoaqipF1L&preST=_SX218_BO1,204,203,200_QL40_&dpSrc=detail)
- [Kruschke (2015) Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan](https://www.amazon.com/Doing-Bayesian-Data-Analysis-Tutorial/dp/0124058884/ref=pd_sim_14_1?_encoding=UTF8&pd_rd_i=0124058884&pd_rd_r=D124M2XTXH7G3SF300RD&pd_rd_w=vLV8u&pd_rd_wg=jNgS3&psc=1&refRID=D124M2XTXH7G3SF300RD&dpID=51LLy0AWDpL&preST=_SX218_BO1,204,203,200_QL40_&dpSrc=detail)
- [Gelman et al. (2014) Bayesian Data Analysis](https://www.amazon.com/Bayesian-Analysis-Chapman-Statistical-Science/dp/1439840954/ref=pd_bxgy_14_img_2?_encoding=UTF8&pd_rd_i=1439840954&pd_rd_r=RBYBCWN0A0E90KM82XAD&pd_rd_w=xo7nR&pd_rd_wg=i2M4F&psc=1&refRID=RBYBCWN0A0E90KM82XAD&dpID=51gfDsQ7vxL&preST=_SY291_BO1,204,203,200_QL40_&dpSrc=detail)


